{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yFVuBymOzdI",
        "outputId": "35e00f77-3572-4c7f-b152-cf27074e53a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [05:02<00:00, 32777.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 160449.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1491674.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3971342.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/50], Step [10/10], Loss: 2.2546\n",
            "Epoch [2/50], Step [10/10], Loss: 1.9598\n",
            "Epoch [3/50], Step [10/10], Loss: 1.3111\n",
            "Epoch [4/50], Step [10/10], Loss: 0.7900\n",
            "Epoch [5/50], Step [10/10], Loss: 0.5558\n",
            "Epoch [6/50], Step [10/10], Loss: 0.4499\n",
            "Epoch [7/50], Step [10/10], Loss: 0.3905\n",
            "Epoch [8/50], Step [10/10], Loss: 0.3473\n",
            "Epoch [9/50], Step [10/10], Loss: 0.2748\n",
            "Epoch [10/50], Step [10/10], Loss: 0.2694\n",
            "Epoch [11/50], Step [10/10], Loss: 0.2369\n",
            "Epoch [12/50], Step [10/10], Loss: 0.1731\n",
            "Epoch [13/50], Step [10/10], Loss: 0.1529\n",
            "Epoch [14/50], Step [10/10], Loss: 0.1443\n",
            "Epoch [15/50], Step [10/10], Loss: 0.1305\n",
            "Epoch [16/50], Step [10/10], Loss: 0.1151\n",
            "Epoch [17/50], Step [10/10], Loss: 0.1176\n",
            "Epoch [18/50], Step [10/10], Loss: 0.1050\n",
            "Epoch [19/50], Step [10/10], Loss: 0.0840\n",
            "Epoch [20/50], Step [10/10], Loss: 0.0769\n",
            "Epoch [21/50], Step [10/10], Loss: 0.0684\n",
            "Epoch [22/50], Step [10/10], Loss: 0.0680\n",
            "Epoch [23/50], Step [10/10], Loss: 0.0468\n",
            "Epoch [24/50], Step [10/10], Loss: 0.0424\n",
            "Epoch [25/50], Step [10/10], Loss: 0.0366\n",
            "Epoch [26/50], Step [10/10], Loss: 0.0407\n",
            "Epoch [27/50], Step [10/10], Loss: 0.0238\n",
            "Epoch [28/50], Step [10/10], Loss: 0.0209\n",
            "Epoch [29/50], Step [10/10], Loss: 0.0138\n",
            "Epoch [30/50], Step [10/10], Loss: 0.0114\n",
            "Epoch [31/50], Step [10/10], Loss: 0.0096\n",
            "Epoch [32/50], Step [10/10], Loss: 0.0085\n",
            "Epoch [33/50], Step [10/10], Loss: 0.0077\n",
            "Epoch [34/50], Step [10/10], Loss: 0.0069\n",
            "Epoch [35/50], Step [10/10], Loss: 0.0059\n",
            "Epoch [36/50], Step [10/10], Loss: 0.0052\n",
            "Epoch [37/50], Step [10/10], Loss: 0.0045\n",
            "Epoch [38/50], Step [10/10], Loss: 0.0043\n",
            "Epoch [39/50], Step [10/10], Loss: 0.0037\n",
            "Epoch [40/50], Step [10/10], Loss: 0.0035\n",
            "Epoch [41/50], Step [10/10], Loss: 0.0035\n",
            "Epoch [42/50], Step [10/10], Loss: 0.0034\n",
            "Epoch [43/50], Step [10/10], Loss: 0.0029\n",
            "Epoch [44/50], Step [10/10], Loss: 0.0027\n",
            "Epoch [45/50], Step [10/10], Loss: 0.0024\n",
            "Epoch [46/50], Step [10/10], Loss: 0.0023\n",
            "Epoch [47/50], Step [10/10], Loss: 0.0024\n",
            "Epoch [48/50], Step [10/10], Loss: 0.0022\n",
            "Epoch [49/50], Step [10/10], Loss: 0.0021\n",
            "Epoch [50/50], Step [10/10], Loss: 0.0019\n",
            "Accuracy of the model on the test images: 91.36%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.conv1(x)))\n",
        "        x = self.pool2(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a subset of the training dataset with 600 samples\n",
        "train_indices = torch.randperm(len(train_dataset))[:600]\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = LeNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/10:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ]
    }
  ]
}