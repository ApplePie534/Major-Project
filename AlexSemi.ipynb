{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubYHCP7rO7fw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(32*12*12,2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024,num),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.feature(x)\n",
        "        x = x.view(-1,32*12*12)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "TIX_S0lUO83y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "VyjAtF-4O_TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training dataset into labeled and unlabeled subsets\n",
        "labeled_indices = torch.randperm(len(train_dataset))[:600]\n",
        "unlabeled_indices = torch.randperm(len(train_dataset))[600:10000] #RAM issues\n",
        "labeled_dataset = Subset(train_dataset, labeled_indices)\n",
        "unlabeled_dataset = Subset(train_dataset, unlabeled_indices)\n",
        "\n",
        "# Create data loaders\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "BvC492vhFp_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, loss function, and optimizer\n",
        "model = AlexNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=1.5, momentum=0.9)"
      ],
      "metadata": {
        "id": "Zciwl6u-PQ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semi-supervised training function\n",
        "def train_semi_supervised(model, labeled_loader, unlabeled_loader, test_loader, epochs, alpha_schedule):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        labeled_loss = 0.0\n",
        "        unlabeled_loss = 0.0\n",
        "\n",
        "        # Train on labeled data\n",
        "        for images, labels in labeled_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            labeled_loss += loss.item()\n",
        "        labeled_loss /= len(labeled_loader)\n",
        "\n",
        "        # # Train on unlabeled data with Pseudo-Labels\n",
        "        # alpha = alpha_schedule(epoch)\n",
        "        # for images, _ in unlabeled_loader:\n",
        "        #     images = images.to(device)\n",
        "        #     optimizer.zero_grad()\n",
        "        #     outputs = model(images)\n",
        "        #     pseudo_labels = torch.max(outputs, 1)[1]\n",
        "        #     unlabeled_loss += alpha * criterion(outputs, pseudo_labels)\n",
        "        # unlabeled_loss /= len(unlabeled_loader)\n",
        "        # # Backpropagate the combined loss\n",
        "        # (labeled_loss + unlabeled_loss).backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Labeled Loss: {labeled_loss:.4f}, Unlabeled Loss: {unlabeled_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "aHvZ58jIPRZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the alpha schedule for semi-supervised learning\n",
        "T1 = 10\n",
        "T2 = 30\n",
        "def alpha_schedule(epoch):\n",
        "    if epoch < T1:\n",
        "        return 0\n",
        "    elif epoch < T2:\n",
        "        return ((epoch - T1) / (T2 - T1)) * 3\n",
        "    else:\n",
        "        return 3"
      ],
      "metadata": {
        "id": "eKz_n39nPYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using semi-supervised learning\n",
        "train_semi_supervised(model, labeled_loader, unlabeled_loader, test_loader, epochs=50, alpha_schedule=alpha_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdIbI6HRPZSF",
        "outputId": "059ddd60-1ffb-4ce5-8e60-9de123e432c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch [1/100], Labeled Loss: 2.3236, Unlabeled Loss: 0.0000, Test Accuracy: 10.28%\n",
            "Epoch [2/100], Labeled Loss: 2.0536, Unlabeled Loss: 0.0000, Test Accuracy: 64.35%\n",
            "Epoch [3/100], Labeled Loss: 0.9603, Unlabeled Loss: 0.0000, Test Accuracy: 76.29%\n",
            "Epoch [4/100], Labeled Loss: 0.7035, Unlabeled Loss: 0.0000, Test Accuracy: 88.32%\n",
            "Epoch [5/100], Labeled Loss: 0.3994, Unlabeled Loss: 0.0000, Test Accuracy: 90.13%\n",
            "Epoch [6/100], Labeled Loss: 0.2485, Unlabeled Loss: 0.0000, Test Accuracy: 91.11%\n",
            "Epoch [7/100], Labeled Loss: 0.2287, Unlabeled Loss: 0.0000, Test Accuracy: 91.56%\n",
            "Epoch [8/100], Labeled Loss: 0.1845, Unlabeled Loss: 0.0000, Test Accuracy: 93.06%\n",
            "Epoch [9/100], Labeled Loss: 0.1421, Unlabeled Loss: 0.0000, Test Accuracy: 93.66%\n",
            "Epoch [10/100], Labeled Loss: 0.1138, Unlabeled Loss: 0.0000, Test Accuracy: 94.56%\n",
            "Epoch [11/100], Labeled Loss: 0.1063, Unlabeled Loss: 0.0000, Test Accuracy: 94.39%\n",
            "Epoch [12/100], Labeled Loss: 0.0887, Unlabeled Loss: 0.0000, Test Accuracy: 94.85%\n",
            "Epoch [13/100], Labeled Loss: 0.0784, Unlabeled Loss: 0.0000, Test Accuracy: 94.84%\n",
            "Epoch [14/100], Labeled Loss: 0.0812, Unlabeled Loss: 0.0000, Test Accuracy: 94.55%\n",
            "Epoch [15/100], Labeled Loss: 0.0811, Unlabeled Loss: 0.0000, Test Accuracy: 94.58%\n",
            "Epoch [16/100], Labeled Loss: 0.1005, Unlabeled Loss: 0.0000, Test Accuracy: 93.77%\n",
            "Epoch [17/100], Labeled Loss: 0.0413, Unlabeled Loss: 0.0000, Test Accuracy: 95.41%\n",
            "Epoch [18/100], Labeled Loss: 0.0343, Unlabeled Loss: 0.0000, Test Accuracy: 95.21%\n",
            "Epoch [19/100], Labeled Loss: 0.0357, Unlabeled Loss: 0.0000, Test Accuracy: 94.63%\n",
            "Epoch [20/100], Labeled Loss: 0.0388, Unlabeled Loss: 0.0000, Test Accuracy: 95.32%\n",
            "Epoch [21/100], Labeled Loss: 0.0426, Unlabeled Loss: 0.0000, Test Accuracy: 95.32%\n",
            "Epoch [22/100], Labeled Loss: 0.0611, Unlabeled Loss: 0.0000, Test Accuracy: 94.74%\n",
            "Epoch [23/100], Labeled Loss: 0.0582, Unlabeled Loss: 0.0000, Test Accuracy: 94.95%\n",
            "Epoch [24/100], Labeled Loss: 0.0245, Unlabeled Loss: 0.0000, Test Accuracy: 93.95%\n",
            "Epoch [25/100], Labeled Loss: 0.0248, Unlabeled Loss: 0.0000, Test Accuracy: 95.07%\n",
            "Epoch [26/100], Labeled Loss: 0.0189, Unlabeled Loss: 0.0000, Test Accuracy: 95.47%\n",
            "Epoch [27/100], Labeled Loss: 0.0332, Unlabeled Loss: 0.0000, Test Accuracy: 95.13%\n",
            "Epoch [28/100], Labeled Loss: 0.0801, Unlabeled Loss: 0.0000, Test Accuracy: 95.83%\n",
            "Epoch [29/100], Labeled Loss: 0.0278, Unlabeled Loss: 0.0000, Test Accuracy: 96.10%\n",
            "Epoch [30/100], Labeled Loss: 0.0168, Unlabeled Loss: 0.0000, Test Accuracy: 95.22%\n",
            "Epoch [31/100], Labeled Loss: 0.0140, Unlabeled Loss: 0.0000, Test Accuracy: 94.72%\n",
            "Epoch [32/100], Labeled Loss: 0.0310, Unlabeled Loss: 0.0000, Test Accuracy: 95.10%\n",
            "Epoch [33/100], Labeled Loss: 0.0953, Unlabeled Loss: 0.0000, Test Accuracy: 95.12%\n",
            "Epoch [34/100], Labeled Loss: 0.0469, Unlabeled Loss: 0.0000, Test Accuracy: 94.04%\n",
            "Epoch [35/100], Labeled Loss: 0.0274, Unlabeled Loss: 0.0000, Test Accuracy: 95.60%\n",
            "Epoch [36/100], Labeled Loss: 0.0105, Unlabeled Loss: 0.0000, Test Accuracy: 95.20%\n",
            "Epoch [37/100], Labeled Loss: 0.0222, Unlabeled Loss: 0.0000, Test Accuracy: 95.43%\n",
            "Epoch [38/100], Labeled Loss: 0.0441, Unlabeled Loss: 0.0000, Test Accuracy: 93.41%\n",
            "Epoch [39/100], Labeled Loss: 0.0360, Unlabeled Loss: 0.0000, Test Accuracy: 95.75%\n",
            "Epoch [40/100], Labeled Loss: 0.0461, Unlabeled Loss: 0.0000, Test Accuracy: 95.33%\n",
            "Epoch [41/100], Labeled Loss: 0.0140, Unlabeled Loss: 0.0000, Test Accuracy: 96.12%\n",
            "Epoch [42/100], Labeled Loss: 0.0069, Unlabeled Loss: 0.0000, Test Accuracy: 95.73%\n",
            "Epoch [43/100], Labeled Loss: 0.0036, Unlabeled Loss: 0.0000, Test Accuracy: 96.14%\n",
            "Epoch [44/100], Labeled Loss: 0.0031, Unlabeled Loss: 0.0000, Test Accuracy: 96.33%\n",
            "Epoch [45/100], Labeled Loss: 0.0208, Unlabeled Loss: 0.0000, Test Accuracy: 96.09%\n",
            "Epoch [46/100], Labeled Loss: 0.0062, Unlabeled Loss: 0.0000, Test Accuracy: 95.86%\n",
            "Epoch [47/100], Labeled Loss: 0.0089, Unlabeled Loss: 0.0000, Test Accuracy: 95.64%\n",
            "Epoch [48/100], Labeled Loss: 0.0030, Unlabeled Loss: 0.0000, Test Accuracy: 95.35%\n",
            "Epoch [49/100], Labeled Loss: 0.0010, Unlabeled Loss: 0.0000, Test Accuracy: 95.40%\n",
            "Epoch [50/100], Labeled Loss: 0.0012, Unlabeled Loss: 0.0000, Test Accuracy: 95.91%\n",
            "Epoch [51/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 95.66%\n",
            "Epoch [52/100], Labeled Loss: 0.0009, Unlabeled Loss: 0.0000, Test Accuracy: 95.96%\n",
            "Epoch [53/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.23%\n",
            "Epoch [54/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.30%\n",
            "Epoch [55/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.30%\n",
            "Epoch [56/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.38%\n",
            "Epoch [57/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 96.45%\n",
            "Epoch [58/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.44%\n",
            "Epoch [59/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 96.39%\n",
            "Epoch [60/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.44%\n",
            "Epoch [61/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.47%\n",
            "Epoch [62/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.48%\n",
            "Epoch [63/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.47%\n",
            "Epoch [64/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.48%\n",
            "Epoch [65/100], Labeled Loss: 0.0003, Unlabeled Loss: 0.0000, Test Accuracy: 96.36%\n",
            "Epoch [66/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.38%\n",
            "Epoch [67/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.49%\n",
            "Epoch [68/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 96.53%\n",
            "Epoch [69/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.56%\n",
            "Epoch [70/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.56%\n",
            "Epoch [71/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 96.53%\n",
            "Epoch [72/100], Labeled Loss: 0.0003, Unlabeled Loss: 0.0000, Test Accuracy: 96.44%\n",
            "Epoch [73/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.51%\n",
            "Epoch [74/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.51%\n",
            "Epoch [75/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.50%\n",
            "Epoch [76/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.51%\n",
            "Epoch [77/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.51%\n",
            "Epoch [78/100], Labeled Loss: 0.0001, Unlabeled Loss: 0.0000, Test Accuracy: 96.54%\n",
            "Epoch [79/100], Labeled Loss: 0.0000, Unlabeled Loss: 0.0000, Test Accuracy: 96.47%\n",
            "Epoch [80/100], Labeled Loss: 0.0009, Unlabeled Loss: 0.0000, Test Accuracy: 96.29%\n",
            "Epoch [81/100], Labeled Loss: 0.0008, Unlabeled Loss: 0.0000, Test Accuracy: 96.20%\n",
            "Epoch [82/100], Labeled Loss: 0.0002, Unlabeled Loss: 0.0000, Test Accuracy: 96.22%\n",
            "Epoch [83/100], Labeled Loss: 0.0003, Unlabeled Loss: 0.0000, Test Accuracy: 96.41%\n",
            "Epoch [84/100], Labeled Loss: 0.0053, Unlabeled Loss: 0.0000, Test Accuracy: 95.92%\n",
            "Epoch [85/100], Labeled Loss: 0.1681, Unlabeled Loss: 0.0000, Test Accuracy: 89.64%\n",
            "Epoch [86/100], Labeled Loss: 0.1985, Unlabeled Loss: 0.0000, Test Accuracy: 92.73%\n",
            "Epoch [87/100], Labeled Loss: 0.1805, Unlabeled Loss: 0.0000, Test Accuracy: 93.28%\n",
            "Epoch [88/100], Labeled Loss: 0.1428, Unlabeled Loss: 0.0000, Test Accuracy: 94.80%\n",
            "Epoch [89/100], Labeled Loss: 0.1126, Unlabeled Loss: 0.0000, Test Accuracy: 95.19%\n",
            "Epoch [90/100], Labeled Loss: 0.0676, Unlabeled Loss: 0.0000, Test Accuracy: 94.85%\n",
            "Epoch [91/100], Labeled Loss: 0.0366, Unlabeled Loss: 0.0000, Test Accuracy: 95.09%\n",
            "Epoch [92/100], Labeled Loss: 0.0357, Unlabeled Loss: 0.0000, Test Accuracy: 95.53%\n",
            "Epoch [93/100], Labeled Loss: 0.0130, Unlabeled Loss: 0.0000, Test Accuracy: 95.36%\n",
            "Epoch [94/100], Labeled Loss: 0.0109, Unlabeled Loss: 0.0000, Test Accuracy: 95.68%\n",
            "Epoch [95/100], Labeled Loss: 0.0185, Unlabeled Loss: 0.0000, Test Accuracy: 93.69%\n",
            "Epoch [96/100], Labeled Loss: 0.0213, Unlabeled Loss: 0.0000, Test Accuracy: 95.20%\n",
            "Epoch [97/100], Labeled Loss: 0.0286, Unlabeled Loss: 0.0000, Test Accuracy: 94.59%\n",
            "Epoch [98/100], Labeled Loss: 0.0114, Unlabeled Loss: 0.0000, Test Accuracy: 94.89%\n",
            "Epoch [99/100], Labeled Loss: 0.0237, Unlabeled Loss: 0.0000, Test Accuracy: 95.28%\n",
            "Epoch [100/100], Labeled Loss: 0.0343, Unlabeled Loss: 0.0000, Test Accuracy: 95.17%\n"
          ]
        }
      ]
    }
  ]
}