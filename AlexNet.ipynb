{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4yFVuBymOzdI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(32*12*12,2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024,num),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.feature(x)\n",
        "        x = x.view(-1,32*12*12)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "EjUfL1b3rHlw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a subset of the training dataset with 600 samples\n",
        "train_indices = torch.randperm(len(train_dataset))[:600]\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGyMjjTSrMqj",
        "outputId": "bdb41580-9190-4303-d86b-d585c27b925e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11224959.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 351646.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3137938.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10171131.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, loss function, and optimizer\n",
        "model = AlexNet()\n",
        "\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ozh0hGXyrNXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17afbf4f-3f1e-48d8-86b0-e6ab83976514"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/10:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtNA7SpkrQVA",
        "outputId": "2ecc2c22-5c19-45e1-e619-cbc077fb3abb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [10/19], Loss: 2.3046\n",
            "Epoch [2/50], Step [10/19], Loss: 1.9411\n",
            "Epoch [3/50], Step [10/19], Loss: 0.8822\n",
            "Epoch [4/50], Step [10/19], Loss: 0.4715\n",
            "Epoch [5/50], Step [10/19], Loss: 0.3441\n",
            "Epoch [6/50], Step [10/19], Loss: 0.1400\n",
            "Epoch [7/50], Step [10/19], Loss: 0.1229\n",
            "Epoch [8/50], Step [10/19], Loss: 0.0864\n",
            "Epoch [9/50], Step [10/19], Loss: 0.1250\n",
            "Epoch [10/50], Step [10/19], Loss: 0.1036\n",
            "Epoch [11/50], Step [10/19], Loss: 0.0480\n",
            "Epoch [12/50], Step [10/19], Loss: 0.0363\n",
            "Epoch [13/50], Step [10/19], Loss: 0.0560\n",
            "Epoch [14/50], Step [10/19], Loss: 0.1560\n",
            "Epoch [15/50], Step [10/19], Loss: 0.0746\n",
            "Epoch [16/50], Step [10/19], Loss: 0.1081\n",
            "Epoch [17/50], Step [10/19], Loss: 0.0909\n",
            "Epoch [18/50], Step [10/19], Loss: 0.0663\n",
            "Epoch [19/50], Step [10/19], Loss: 0.0281\n",
            "Epoch [20/50], Step [10/19], Loss: 0.0780\n",
            "Epoch [21/50], Step [10/19], Loss: 0.0338\n",
            "Epoch [22/50], Step [10/19], Loss: 0.0486\n",
            "Epoch [23/50], Step [10/19], Loss: 0.0315\n",
            "Epoch [24/50], Step [10/19], Loss: 0.0250\n",
            "Epoch [25/50], Step [10/19], Loss: 0.0052\n",
            "Epoch [26/50], Step [10/19], Loss: 0.0688\n",
            "Epoch [27/50], Step [10/19], Loss: 0.0776\n",
            "Epoch [28/50], Step [10/19], Loss: 0.0567\n",
            "Epoch [29/50], Step [10/19], Loss: 0.0578\n",
            "Epoch [30/50], Step [10/19], Loss: 0.0498\n",
            "Epoch [31/50], Step [10/19], Loss: 0.0602\n",
            "Epoch [32/50], Step [10/19], Loss: 0.0205\n",
            "Epoch [33/50], Step [10/19], Loss: 0.0112\n",
            "Epoch [34/50], Step [10/19], Loss: 0.0184\n",
            "Epoch [35/50], Step [10/19], Loss: 0.0277\n",
            "Epoch [36/50], Step [10/19], Loss: 0.0060\n",
            "Epoch [37/50], Step [10/19], Loss: 0.0036\n",
            "Epoch [38/50], Step [10/19], Loss: 0.0065\n",
            "Epoch [39/50], Step [10/19], Loss: 0.0281\n",
            "Epoch [40/50], Step [10/19], Loss: 0.1037\n",
            "Epoch [41/50], Step [10/19], Loss: 0.0592\n",
            "Epoch [42/50], Step [10/19], Loss: 0.0068\n",
            "Epoch [43/50], Step [10/19], Loss: 0.0021\n",
            "Epoch [44/50], Step [10/19], Loss: 0.0317\n",
            "Epoch [45/50], Step [10/19], Loss: 0.0438\n",
            "Epoch [46/50], Step [10/19], Loss: 0.1195\n",
            "Epoch [47/50], Step [10/19], Loss: 0.0132\n",
            "Epoch [48/50], Step [10/19], Loss: 0.0337\n",
            "Epoch [49/50], Step [10/19], Loss: 0.0353\n",
            "Epoch [50/50], Step [10/19], Loss: 0.0069\n",
            "Accuracy of the model on the test images: 94.86%\n"
          ]
        }
      ]
    }
  ]
}